@misc{ubs2023latest,
  author    = {UBS},
  title     = {{Latest House View Daily}},
  year      = {2023},
  url       = {https://www.ubs.com/global/en/wealthmanagement/insights/chief-investment-office/house-view/daily/2023/latest-25052023.html},
}

@misc{modelcontextprotocol2024,
  author    = {Model Context Protocol},
  title     = {{Getting Started - Introduction}},
  year      = {2024},
  url       = {https://modelcontextprotocol.io/docs/getting-started/intro},
}

@article{goodman2001progress,
  author = {Goodman, Joshua},
  title = {{A Bit of Progress in Language Modeling}},
  year = {2001},
  journal = {arXiv preprint},
  eprint = {cs/0108005},
  url = {https://arxiv.org/abs/cs/0108005}
}

@article{kilgarriff2003webascorpus,
  author = {Kilgarriff, Adam and Grefenstette, Gregory},
  title = {{Introduction to the Special Issue on the Web as Corpus}},
  journal = {Computational Linguistics},
  volume = {29},
  number = {3},
  pages = {333--347},
  year = {2003},
  doi = {10.1162/089120103322711569},
  issn = {0891-2017}
}

@inproceedings{banko2001scaling,
  author = {Banko, Michele and Brill, Eric},
  title = {{Scaling to very very large corpora for natural language disambiguation}},
  booktitle = {Proceedings of the 39th Annual Meeting on Association for Computational Linguistics},
  year = {2001},
  pages = {26--33},
  doi = {10.3115/1073012.1073017},
  publisher = {Association for Computational Linguistics}
}

@article{resnik2003webparallel,
  author = {Resnik, Philip and Smith, Noah A.},
  title = {{The Web as a Parallel Corpus}},
  journal = {Computational Linguistics},
  volume = {29},
  number = {3},
  pages = {349--380},
  year = {2003},
  doi = {10.1162/089120103322711578},
  issn = {0891-2017},
  note = {Archived from the original on 2024-06-07. Retrieved 2024-06-07}
}

@inproceedings{xu2000annlm,
  author = {Xu, Wei and Rudnicky, Alex},
  title = {{Can artificial neural networks learn language models?}},
  booktitle = {6th International Conference on Spoken Language Processing (ICSLP 2000)},
  volume = {1},
  year = {2000},
  publisher = {ISCA},
  doi = {10.21437/icslp.2000-50}
}

@article{chen2021cnnreview,
  author = {Chen, Leiyu and Li, Shaobo and Bai, Qiang and Yang, Jing and Jiang, Sanlong and Miao, Yanming},
  title = {{Review of Image Classification Algorithms Based on Convolutional Neural Networks}},
  journal = {Remote Sensing},
  volume = {13},
  number = {22},
  pages = {4712},
  year = {2021},
  doi = {10.3390/rs13224712},
  bibcode = {2021RemS...13.4712C}
}

@inproceedings{vaswani2017attention,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  title = {{Attention is All you Need}},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {30},
  year = {2017},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
  note = {Archived (PDF) from the original on 2024-02-21. Retrieved 2024-01-21}
}

@article{bahdanau2014nmt,
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
  year = {2014},
  journal = {arXiv preprint},
  eprint = {1409.0473},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/1409.0473}
}

@article{rogers2020bertology,
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  title = {{A Primer in BERTology: What We Know About How BERT Works}},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {842--866},
  year = {2020},
  doi = {10.1162/tacl_a_00349},
  eprint = {2002.12327},
  archivePrefix = {arXiv},
  s2cid = {211532403},
  note = {Archived from the original on 2022-04-03. Retrieved 2024-01-21}
}

@inproceedings{movva2024topics,
  author = {Movva, Rajiv and Balachandar, Sidhika and Peng, Kenny and Agostini, Gabriel and Garg, Nikhil and Pierson, Emma},
  title = {{Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers}},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages = {1223--1243},
  year = {2024},
  doi = {10.18653/v1/2024.naacl-long.67},
  eprint = {2307.10700},
  archivePrefix = {arXiv},
  note = {Retrieved 2024-12-08}
}

@article{hern2019fakeai,
  author = {Hern, Alex},
  title = {{New AI fake text generator may be too dangerous to release, say creators}},
  journal = {The Guardian},
  year = {2019},
  month = {February},
  day = {14},
  url = {https://web.archive.org/web/20190214173112/https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction},
  note = {Archived from the original on 14 February 2019. Retrieved 20 January 2024}
}

@article{euronews2023chatgpt,
  author = {Euronews},
  title = {{ChatGPT a year on: 3 ways the AI chatbot has completely changed the world in 12 months}},
  journal = {Euronews},
  year = {2023},
  month = {November},
  day = {30},
  url = {https://www.euronews.com/next/2023/11/30/chatgpt-a-year-on-3-ways-the-ai-chatbot-has-completely-changed-the-world-in-12-months},
  note = {Archived from the original on January 14, 2024. Retrieved January 20, 2024}
}

@article{heaven2023gpt4,
  author = {Heaven, Will},
  title = {{GPT-4 is bigger and better than ChatGPT—but OpenAI won't say why}},
  journal = {MIT Technology Review},
  year = {2023},
  month = {March},
  day = {14},
  url = {https://web.archive.org/web/20230317224201/https://www.technologyreview.com/2023/03/14/1069823/gpt-4-is-bigger-and-better-chatgpt-openai/},
  note = {Archived from the original on March 17, 2023. Retrieved January 20, 2024}
}

@article{metz2024openai,
  author = {Metz, Cade},
  title = {{OpenAI Unveils New ChatGPT That Can Reason Through Math and Science}},
  journal = {The New York Times},
  year = {2024},
  month = {September},
  day = {12},
  url = {https://www.nytimes.com/2024/09/12/technology/openai-chatgpt-math.html},
  note = {Retrieved September 12, 2024}
}

@misc{ourworldindata2023parameters,
  title = {{Parameters in notable artificial intelligence systems}},
  year = {2023},
  month = {November},
  day = {30},
  url = {https://ourworldindata.org/grapher/artificial-intelligence-parameter-count?time=2017-09-05..latest},
  note = {Retrieved January 20, 2024}
}

@article{sharma2025deepseek,
  author = {Sharma, Shubham},
  title = {{Open-source DeepSeek-R1 uses pure reinforcement learning to match OpenAI o1 — at 95\% less cost}},
  journal = {VentureBeat},
  year = {2025},
  month = {January},
  day = {20},
  url = {https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/},
  note = {Retrieved 2025-01-26}
}

@article{zia2024multimodal,
  author = {Zia, Dr Tehseen},
  title = {{Unveiling of Large Multimodal Models: Shaping the Landscape of Language Models in 2024}},
  journal = {Unite.AI},
  year = {2024},
  month = {January},
  day = {8},
  url = {https://www.unite.ai/unveiling-of-large-multimodal-models-shaping-the-landscape-of-language-models-in-2024/},
  note = {Retrieved 2024-12-28}
}

@misc{merritt2022transformer,
  author = {Merritt, Rick},
  title = {{What Is a Transformer Model?}},
  howpublished = {NVIDIA Blog},
  year = {2022},
  month = {March},
  day = {25},
  url = {https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/},
  note = {Archived from the original on 2023-11-17. Retrieved 2023-07-25}
}

@misc{modelcontextprotocol2024intro,
  author    = {Model Context Protocol},
  title     = {{Getting Started - Introduction}},
  year      = {2024},
  url       = {https://modelcontextprotocol.io/docs/getting-started/intro},
  note      = {Accessed: 2025-08-16}
}

@misc{modelcontextprotocol2024arch,
  author    = {Model Context Protocol},
  title     = {{Learn - Architecture}},
  year      = {2024},
  url       = {https://modelcontextprotocol.io/docs/learn/architecture},
  note      = {Accessed: 2025-08-16}
}

@misc{LLM_next_token_prediction,
  title = {{Introduction to Large Language Models}},
  author = {{Google Developers}},
  year = {2024},
  url = {https://developers.google.com/machine-learning/resources/intro-llms}
}

@misc{tokens_tokenization,
  title = {{LLM Transformer Model Visually Explained}},
  author = {Polo Club},
  year = {2024},
  url = {https://poloclub.github.io/transformer-explainer/}
}

@misc{LLM_fine_tuning,
  title = {{What are Large Language Models (LLMs)?}},
  author = {IBM},
  year = {2023},
  url = {https://www.ibm.com/think/topics/large-language-models}
}

@misc{transformer_self_attention,
  title = {{Transformer (deep learning architecture)}},
  author = {{Wikipedia contributors}},
  year = {2025},
  url = {https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)}
}

@misc{attention_human_cognition,
  title = {{What is an attention mechanism?}},
  author = {IBM},
  year = {2024},
  url = {https://www.ibm.com/think/topics/attention-mechanism}
}

@misc{transformer_parallelizable,
  title = {{Attention Is All You Need}},
  author = {{Wikipedia contributors}},
  year = {2025},
  url = {https://en.wikipedia.org/wiki/Attention_Is_All_You_Need}
}

@misc{fine_tuning_transfer_learning,
  title = {{What is fine-tuning?}},
  author = {Dave Bergmann (IBM)},
  year = {2024},
  url = {https://www.ibm.com/think/topics/fine-tuning}
}

@misc{RLHF_reward_model_from_humans,
  title = {{Reinforcement learning from human feedback}},
  author = {{Wikipedia contributors}},
  year = {2025},
  url = {https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback}
}

@misc{azurefunctions_msdocs,
  author = {{Microsoft Docs}},
  title = {{Azure Functions overview}},
  url = {https://learn.microsoft.com/it-it/azure/azure-functions/functions-overview?pivots=programming-language-csharp},
  note = {Accessed: 2025-08-16}
}

@inproceedings{chernyavskiy2021transformers,
  author    = {Chernyavskiy, Andrey and Ilvovsky, Dmitry and Nakov, Preslav},
  title     = {Transformers: "the end of history" for natural language processing?},
  booktitle = {Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13--17, 2021, Proceedings, Part III},
  series    = {Lecture Notes in Computer Science},
  volume    = {12977},
  pages     = {677--693},
  publisher = {Springer},
  year      = {2021},
  doi       = {10.1007/978-3-030-86523-8_41},
  url       = {https://doi.org/10.1007/978-3-030-86523-8_41},
  note      = {Accessed: 2025-09-02}
}

@misc{devlin2019bert,
  author       = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title        = {BERT: Pre-training of deep bidirectional transformers for language understanding},
  year         = {2018},
  eprint       = {arXiv:1810.04805},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/1810.04805},
  note         = {Accessed: 2025-09-02}
}

@inproceedings{peters2018elmo,
  author    = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  title     = {Deep contextualized word representations},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  pages     = {2227--2237},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://aclanthology.org/N18-1202},
  note      = {Accessed: 2025-09-02}
}

@misc{lewis2019bart,
  author       = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  title        = {BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  year         = {2019},
  eprint       = {arXiv:1910.13461},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/1910.13461},
  note         = {Accessed: 2025-09-02}
}

@misc{chung2022scaling,
  author       = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Saurabh and others},
  title        = {Scaling instruction-finetuned language models},
  year         = {2022},
  eprint       = {arXiv:2210.11416},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2210.11416},
  note         = {Accessed: 2025-09-02}
}

@misc{sanh2021multitask,
  author       = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Abheesht and others},
  title        = {Multitask prompted training enables zero-shot task generalization},
  year         = {2021},
  eprint       = {arXiv:2110.08207},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2110.08207},
  note         = {Accessed: 2025-09-02}
}

@inproceedings{wang2022supernaturalinstructions,
  author    = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amir and Naik, Aashka and Ashok, Anjana and Dhanasekaran, Abhinav Suresh and Arunkumar, Anmol and Stap, David and others},
  title     = {Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {5085--5109},
  year      = {2022},
  url       = {https://aclanthology.org/2022.emnlp-main.341},
  note      = {Accessed: 2025-09-02}
}

@misc{wang2022selfinstruct,
  author       = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  title        = {Self-Instruct: Aligning language model with self generated instructions},
  year         = {2022},
  eprint       = {arXiv:2212.10560},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2212.10560},
  note         = {Accessed: 2025-09-02}
}

@inproceedings{ouyang2022rlhf,
  author    = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  title     = {Training language models to follow instructions with human feedback},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {35},
  pages     = {27730--27744},
  year      = {2022},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
  note      = {Accessed: 2025-09-02}
}

@misc{touvron2023llama2,
  author       = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Sharan and Bhargava, Puneet and Bhosale, Shruti and others},
  title        = {LLaMA 2: Open foundation and fine-tuned chat models},
  year         = {2023},
  eprint       = {arXiv:2307.09288},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2307.09288},
  note         = {Accessed: 2025-09-02}
}

@misc{wei2022emergent,
  author       = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  title        = {Emergent abilities of large language models},
  year         = {2022},
  eprint       = {arXiv:2206.07682},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2206.07682},
  note         = {Accessed: 2025-09-02}
}

@article{webb2023emergent,
  author    = {Webb, Taylor and Holyoak, Keith J. and Lu, Hongjing},
  title     = {Emergent analogical reasoning in large language models},
  journal   = {Nature Human Behaviour},
  volume    = {7},
  number    = {9},
  pages     = {1526--1541},
  year      = {2023},
  doi       = {10.1038/s41562-023-01659-w},
  url       = {https://doi.org/10.1038/s41562-023-01659-w},
  note      = {Accessed: 2025-09-02}
}

@misc{boiko2023emergent,
  author       = {Boiko, Daniil A. and MacKnight, Robert and Gomes, Gabriel},
  title        = {Emergent autonomous scientific research capabilities of large language models},
  year         = {2023},
  eprint       = {arXiv:2304.05332},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2304.05332},
  note         = {Accessed: 2025-09-02}
}

@misc{microsoft_webapi_ef_part5,
  author       = {{Microsoft Docs}},
  title        = {{Using Web API with Entity Framework (Part 5)}},
  howpublished = {Microsoft Docs},
  url          = {https://learn.microsoft.com/it-it/aspnet/web-api/overview/data/using-web-api-with-entity-framework/part-5},
  note         = {Accessed: 2025-09-02}
}

@misc{azure_cqrs_msdocs,
  author       = {{Microsoft Docs}},
  title        = {{CQRS pattern}},
  howpublished = {Microsoft Docs},
  url          = {https://learn.microsoft.com/it-it/azure/architecture/patterns/cqrs},
  note         = {Accessed: 2025-09-02}
}

@misc{progesoftware_3fs,
  author = {{ProgeSoftware}},
  title  = {{3FS - ProgeSoftware}},
  year   = {2021},
  url    = {https://3fs.progesoftware.it/},
  note   = {Accessed: 2025-09-02}
}

@misc{arxiv230706435,
  author       = {Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
  title        = {{arXiv preprint arXiv:2307.06435}},
  year         = {2023},
  eprint       = {2307.06435},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2307.06435},
  note         = {Accessed: 2025-09-02}
}

@inproceedings{webster1992tokenization,
  author    = {Webster, J. J. and Kit, C.},
  title     = {{Tokenization as the initial phase in NLP}},
  booktitle = {Proceedings of the 14th International Conference on Computational Linguistics (COLING), Volume 4},
  year      = {1992}
}

@inproceedings{kudo2018subword,
  author    = {Kudo, Taku},
  title     = {{Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates}},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year      = {2018},
  pages     = {66--75},
  url       = {https://aclanthology.org/P18-1007}
}

@inproceedings{sennrich2016neural,
  author    = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  title     = {{Neural Machine Translation of Rare Words with Subword Units}},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year      = {2016},
  pages     = {1715--1725},
  url       = {https://aclanthology.org/P16-1162}
}

@article{fedus2022switch,
  author  = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  title   = {{Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}},
  journal = {Journal of Machine Learning Research},
  volume  = {23},
  number  = {1},
  pages   = {5232--5270},
  year    = {2022}
}

@inproceedings{du2022glam,
  author    = {Du, N. and Huang, Y. and Dai, A. M. and Tong, S. and Lepikhin, D. and Xu, Y. and Krikun, M. and Zhou, Y. and Yu, A. W. and Firat, O. and others},
  title     = {{GLAM: Efficient Scaling of Language Models with Mixture-of-Experts}},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  pages     = {5547--5569},
  year      = {2022},
  publisher = {PMLR}
}

@misc{ren2023pangu-p,
  author       = {Ren, X. and Zhou, P. and Meng, X. and Huang, X. and Wang, Y. and Wang, W. and Li, P. and Zhang, X. and Podolskiy, A. and Arshinov, G. and others},
  title        = {{Pangu-P: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing}},
  year         = {2023},
  eprint       = {arXiv:2303.10845},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2303.10845}
}

@inproceedings{wang2022what,
  author    = {Wang, Tao and Roberts, Adam and Hesslow, David and Le Scao, Teven and Chung, Hyung Won and Beltagy, Iz and Launay, Jérémy and Raffel, Colin},
  title     = {{What Language Model Architecture and Pretraining Objective Works Best for Zero-Shot Generalization?}},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  pages     = {22964--22984},
  year      = {2022},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v162/wang22a.html}
}

@article{kaplan2020scaling,
  author       = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeff and Amodei, Dario},
  title        = {{Scaling Laws for Neural Language Models}},
  journal      = {arXiv preprint},
  year         = {2020},
  eprint       = {2001.08361},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2001.08361}
}

@article{hoffmann2022training,
  author       = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Aran and Buchatskaya, Elena and Cai, Trevor and Rutherford, Ethan and de Lara Casas, Diego and Hendricks, Laura A. and Welbl, Jonathan and Clark, Jack and others},
  title        = {{Training Compute-Optimal Large Language Models}},
  journal      = {arXiv preprint},
  year         = {2022},
  eprint       = {2203.15556},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2203.15556}
}

@article{ziegler2019fine,
  author       = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  title        = {Fine-Tuning Language Models from Human Preferences},
  journal      = {arXiv preprint},
  year         = {2019},
  eprint       = {1909.08593},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/1909.08593}
}

@misc{dong2023survey,
  author       = {Dong, Q. and Li, L. and Dai, D. and Zheng, C. and Wu, Z. and Chang, B. and Sun, X. and Xu, J. and Sui, Z.},
  title        = {A Survey for In-Context Learning},
  year         = {2023},
  eprint       = {2301.00234},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2301.00234}
}

@inproceedings{wei2022chain,
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny and others},
  title     = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {35},
  pages     = {24824--24837},
  year      = {2022}
}

@misc{wang2022selfconsistency,
  author       = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc V. and Chi, Ed H. and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  title        = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  year         = {2022},
  eprint       = {2203.11171},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2203.11171}
}

@misc{yao2023tree,
  author       = {Yao, S. and Yu, D. and Zhao, J. and Shafran, I. and Griffiths, T. L. and Cao, Y. and Narasimhan, K.},
  title        = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  year         = {2023},
  eprint       = {2305.10601},
  archivePrefix= {arXiv},
  primaryClass = {cs.AI},
  url          = {https://arxiv.org/abs/2305.10601},
  note         = {Accessed: 2025-09-02}
}

@article{hendrycks2020measuring,
  author       = {Hendrycks, Dan and Burns, Christopher and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  title        = {{Measuring massive multitask language understanding}},
  journal      = {arXiv preprint},
  year         = {2020},
  eprint       = {2009.03300},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2009.03300}
}

@article{wang2018glue,
  author       = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  title        = {{GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}},
  journal      = {arXiv preprint},
  year         = {2018},
  eprint       = {1804.07461},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/1804.07461}
}

@inproceedings{wang2019superglue,
  author       = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  title        = {{SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems}},
  booktitle    = {Advances in Neural Information Processing Systems},
  year         = {2019},
  eprint       = {1905.00537},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/1905.00537}
}

@article{srivastava2022beyond,
  author       = {Srivastava, A. and Rastogi, A. and Rao, A. and Shoeb, A. A. M. and Abid, A. and Fisch, A. and Brown, A. R. and Santoro, A. and Gupta, A. and Garriga-Alonso, A. and others},
  title        = {{Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models}},
  journal      = {arXiv preprint},
  year         = {2022},
  eprint       = {2206.04615},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2206.04615}
}

@article{clark2018arc,
  author       = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tal and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  title        = {{Think You Have Solved Question Answering? Try ARC, The AI2 Reasoning Challenge}},
  journal      = {arXiv preprint},
  year         = {2018},
  eprint       = {1803.05457},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/1803.05457}
}

@article{lai2017race,
  author       = {Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  title        = {{RACE: Large-scale Reading Comprehension Dataset from Examinations}},
  journal      = {arXiv preprint},
  year         = {2017},
  eprint       = {1704.04683},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/1704.04683}
}

@article{rajpurkar2016squad,
  author       = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  title        = {{SQuAD: 100,000+ Questions for Machine Comprehension of Text}},
  journal      = {arXiv preprint},
  year         = {2016},
  eprint       = {1606.05250},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/1606.05250}
}

@article{clark2019boolq,
  author       = {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  title        = {{BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions}},
  journal      = {arXiv preprint},
  year         = {2019},
  eprint       = {1905.10044},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/1905.10044}
}

@article{cobbe2021training,
  author       = {Cobbe, K. and Kosaraju, V. and Bavarian, M. and Chen, M. and Jun, H. and Kaiser, L. and Plappert, M. and Tworek, J. and Hilton, J. and Nakano, R. and others},
  title        = {{Training verifiers to solve math word problems}},
  journal      = {arXiv preprint},
  year         = {2021},
  eprint       = {2110.14168},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2110.14168}
}

@article{chen2021evaluating,
  author       = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiao and Pinto, Henrique P. de Oliveira and Kaplan, Jared and Edwards, Harold and Burda, Yury and Joseph, Nicholas and Brockman, Greg and others},
  title        = {{Evaluating Large Language Models Trained on Code}},
  journal      = {arXiv preprint},
  year         = {2021},
  eprint       = {2107.03374},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2107.03374}
}

@article{wei2023jailbroken,
  author       = {Wei, A. and Haghtalab, N. and Steinhardt, J.},
  title        = {{Jailbroken: How Does LLM Safety Training Fail?}},
  journal      = {arXiv preprint},
  year         = {2023},
  eprint       = {2307.02483},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2307.02483}
}

@article{ganguli2022redteaming,
  author       = {Ganguli, D. and Lovitt, L. and Kernion, J. and Askell, A. and Bai, Y. and Kadavath, S. and Mann, B. and Perez, E. and Schiefer, N. and Ndousse, K. and others},
  title        = {{Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned}},
  journal      = {arXiv preprint},
  year         = {2022},
  eprint       = {2209.07858},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2209.07858}
}

@article{casper2023explore,
  author       = {Casper, S. and Lin, J. and Kwon, J. and Culp, G. and Hadfield-Menell, D.},
  title        = {{Explore, Establish, Exploit: Red Teaming Language Models from Scratch}},
  journal      = {arXiv preprint},
  year         = {2023},
  eprint       = {2306.09442},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2306.09442}
}

@article{perez2022redwithlm,
  author       = {Perez, E. and Huang, S. and Song, F. and Cai, T. and Ring, R. and Aslanides, J. and Glaese, A. and McAleese, N. and Irving, G.},
  title        = {{Red Teaming Language Models with Language Models}},
  journal      = {arXiv preprint},
  year         = {2022},
  eprint       = {2202.03286},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2202.03286}
}

@misc{msdn2009separation,
  author = {Brownfield, MSDN Magazine},
  title = {Separation of Concerns: A Brownfield Development Series},
  year = {2009},
  url = {https://learn.microsoft.com/en-us/archive/msdn-magazine/2009/brownfield/separation-of-concerns-a-brownfield-development-series},
  note = {Accessed: 2025-09-25}
}

@misc{azure_openai_foundry,
  author = {{Microsoft Docs}},
  title = {What is Azure AI Foundry for OpenAI?},
  year = {2024},
  url = {https://learn.microsoft.com/en-us/azure/ai-foundry/openai/overview},
  note = {Accessed: 2025-09-26}
}

@misc{ibm_context_window,
  author = {{IBM}},
  title = {What is a context window?},
  year = {2024},
  url = {https://www.ibm.com/think/topics/context-window},
  note = {Accessed: 2025-09-26}
}