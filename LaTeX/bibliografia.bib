@misc{ubs2023latest,
  author    = {UBS},
  title     = {{Latest House View Daily}},
  year      = {2023},
  url       = {https://www.ubs.com/global/en/wealthmanagement/insights/chief-investment-office/house-view/daily/2023/latest-25052023.html},
}

@misc{modelcontextprotocol2023,
  author    = {Model Context Protocol},
  title     = {{Getting Started - Introduction}},
  year      = {2023},
  url       = {https://modelcontextprotocol.io/docs/getting-started/intro},
}

@article{goodman2001progress,
  author = {Goodman, Joshua},
  title = {{A Bit of Progress in Language Modeling}},
  year = {2001},
  journal = {arXiv preprint},
  eprint = {cs/0108005},
  url = {https://arxiv.org/abs/cs/0108005}
}

@article{kilgarriff2003webascorpus,
  author = {Kilgarriff, Adam and Grefenstette, Gregory},
  title = {{Introduction to the Special Issue on the Web as Corpus}},
  journal = {Computational Linguistics},
  volume = {29},
  number = {3},
  pages = {333--347},
  year = {2003},
  doi = {10.1162/089120103322711569},
  issn = {0891-2017}
}

@inproceedings{banko2001scaling,
  author = {Banko, Michele and Brill, Eric},
  title = {{Scaling to very very large corpora for natural language disambiguation}},
  booktitle = {Proceedings of the 39th Annual Meeting on Association for Computational Linguistics},
  year = {2001},
  pages = {26--33},
  doi = {10.3115/1073012.1073017},
  publisher = {Association for Computational Linguistics}
}

@article{resnik2003webparallel,
  author = {Resnik, Philip and Smith, Noah A.},
  title = {{The Web as a Parallel Corpus}},
  journal = {Computational Linguistics},
  volume = {29},
  number = {3},
  pages = {349--380},
  year = {2003},
  doi = {10.1162/089120103322711578},
  issn = {0891-2017},
  note = {Archived from the original on 2024-06-07. Retrieved 2024-06-07}
}

@inproceedings{xu2000annlm,
  author = {Xu, Wei and Rudnicky, Alex},
  title = {{Can artificial neural networks learn language models?}},
  booktitle = {6th International Conference on Spoken Language Processing (ICSLP 2000)},
  volume = {1},
  year = {2000},
  publisher = {ISCA},
  doi = {10.21437/icslp.2000-50}
}

@article{chen2021cnnreview,
  author = {Chen, Leiyu and Li, Shaobo and Bai, Qiang and Yang, Jing and Jiang, Sanlong and Miao, Yanming},
  title = {{Review of Image Classification Algorithms Based on Convolutional Neural Networks}},
  journal = {Remote Sensing},
  volume = {13},
  number = {22},
  pages = {4712},
  year = {2021},
  doi = {10.3390/rs13224712},
  bibcode = {2021RemS...13.4712C}
}

@inproceedings{vaswani2017attention,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  title = {{Attention is All you Need}},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {30},
  year = {2017},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
  note = {Archived (PDF) from the original on 2024-02-21. Retrieved 2024-01-21}
}

@article{bahdanau2014nmt,
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
  year = {2014},
  journal = {arXiv preprint},
  eprint = {1409.0473},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/1409.0473}
}

@article{rogers2020bertology,
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  title = {{A Primer in BERTology: What We Know About How BERT Works}},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {842--866},
  year = {2020},
  doi = {10.1162/tacl_a_00349},
  eprint = {2002.12327},
  archivePrefix = {arXiv},
  s2cid = {211532403},
  note = {Archived from the original on 2022-04-03. Retrieved 2024-01-21}
}

@inproceedings{movva2024topics,
  author = {Movva, Rajiv and Balachandar, Sidhika and Peng, Kenny and Agostini, Gabriel and Garg, Nikhil and Pierson, Emma},
  title = {{Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers}},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages = {1223--1243},
  year = {2024},
  doi = {10.18653/v1/2024.naacl-long.67},
  eprint = {2307.10700},
  archivePrefix = {arXiv},
  note = {Retrieved 2024-12-08}
}

@article{hern2019fakeai,
  author = {Hern, Alex},
  title = {{New AI fake text generator may be too dangerous to release, say creators}},
  journal = {The Guardian},
  year = {2019},
  month = {February},
  day = {14},
  url = {https://web.archive.org/web/20190214173112/https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction},
  note = {Archived from the original on 14 February 2019. Retrieved 20 January 2024}
}

@article{euronews2023chatgpt,
  author = {Euronews},
  title = {{ChatGPT a year on: 3 ways the AI chatbot has completely changed the world in 12 months}},
  journal = {Euronews},
  year = {2023},
  month = {November},
  day = {30},
  url = {https://www.euronews.com/next/2023/11/30/chatgpt-a-year-on-3-ways-the-ai-chatbot-has-completely-changed-the-world-in-12-months},
  note = {Archived from the original on January 14, 2024. Retrieved January 20, 2024}
}

@article{heaven2023gpt4,
  author = {Heaven, Will},
  title = {{GPT-4 is bigger and better than ChatGPT—but OpenAI won't say why}},
  journal = {MIT Technology Review},
  year = {2023},
  month = {March},
  day = {14},
  url = {https://web.archive.org/web/20230317224201/https://www.technologyreview.com/2023/03/14/1069823/gpt-4-is-bigger-and-better-chatgpt-openai/},
  note = {Archived from the original on March 17, 2023. Retrieved January 20, 2024}
}

@article{metz2024openai,
  author = {Metz, Cade},
  title = {{OpenAI Unveils New ChatGPT That Can Reason Through Math and Science}},
  journal = {The New York Times},
  year = {2024},
  month = {September},
  day = {12},
  url = {https://www.nytimes.com/2024/09/12/technology/openai-chatgpt-math.html},
  note = {Retrieved September 12, 2024}
}

@misc{ourworldindata2023parameters,
  title = {{Parameters in notable artificial intelligence systems}},
  year = {2023},
  month = {November},
  day = {30},
  url = {https://ourworldindata.org/grapher/artificial-intelligence-parameter-count?time=2017-09-05..latest},
  note = {Retrieved January 20, 2024}
}

@article{sharma2025deepseek,
  author = {Sharma, Shubham},
  title = {{Open-source DeepSeek-R1 uses pure reinforcement learning to match OpenAI o1 — at 95\% less cost}},
  journal = {VentureBeat},
  year = {2025},
  month = {January},
  day = {20},
  url = {https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/},
  note = {Retrieved 2025-01-26}
}

@article{zia2024multimodal,
  author = {Zia, Dr Tehseen},
  title = {{Unveiling of Large Multimodal Models: Shaping the Landscape of Language Models in 2024}},
  journal = {Unite.AI},
  year = {2024},
  month = {January},
  day = {8},
  url = {https://www.unite.ai/unveiling-of-large-multimodal-models-shaping-the-landscape-of-language-models-in-2024/},
  note = {Retrieved 2024-12-28}
}

@misc{merritt2022transformer,
  author = {Merritt, Rick},
  title = {{What Is a Transformer Model?}},
  howpublished = {NVIDIA Blog},
  year = {2022},
  month = {March},
  day = {25},
  url = {https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/},
  note = {Archived from the original on 2023-11-17. Retrieved 2023-07-25}
}

@misc{modelcontextprotocol2023intro,
  author    = {Model Context Protocol},
  title     = {{Getting Started - Introduction}},
  year      = {2023},
  url       = {https://modelcontextprotocol.io/docs/getting-started/intro},
  note      = {Accessed: 2025-08-16}
}

@misc{modelcontextprotocol2023arch,
  author    = {Model Context Protocol},
  title     = {{Learn - Architecture}},
  year      = {2023},
  url       = {https://modelcontextprotocol.io/docs/learn/architecture},
  note      = {Accessed: 2025-08-16}
}

@misc{LLM_next_token_prediction,
  title = {{Introduction to Large Language Models}},
  author = {{Google Developers}},
  year = {2024},
  url = {https://developers.google.com/machine-learning/resources/intro-llms}
}

@misc{tokens_tokenization,
  title = {{LLM Transformer Model Visually Explained}},
  author = {Polo Club},
  year = {2024},
  url = {https://poloclub.github.io/transformer-explainer/}
}

@misc{LLM_fine_tuning,
  title = {{What are Large Language Models (LLMs)?}},
  author = {IBM},
  year = {2023},
  url = {https://www.ibm.com/think/topics/large-language-models}
}

@misc{transformer_self_attention,
  title = {{Transformer (deep learning architecture)}},
  author = {{Wikipedia contributors}},
  year = {2025},
  url = {https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)}
}

@misc{attention_human_cognition,
  title = {{What is an attention mechanism?}},
  author = {IBM},
  year = {2024},
  url = {https://www.ibm.com/think/topics/attention-mechanism}
}

@misc{transformer_parallelizable,
  title = {{Attention Is All You Need}},
  author = {{Wikipedia contributors}},
  year = {2025},
  url = {https://en.wikipedia.org/wiki/Attention_Is_All_You_Need}
}

@misc{fine_tuning_transfer_learning,
  title = {{What is fine-tuning?}},
  author = {Dave Bergmann (IBM)},
  year = {2024},
  url = {https://www.ibm.com/think/topics/fine-tuning}
}

@misc{RLHF_reward_model_from_humans,
  title = {{Reinforcement learning from human feedback}},
  author = {{Wikipedia contributors}},
  year = {2025},
  url = {https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback}
}
