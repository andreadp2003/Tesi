@misc{ubs2023latest,
  author    = {UBS},
  title     = {{Latest House View Daily}},
  year      = {2023},
  url       = {https://www.ubs.com/global/en/wealthmanagement/insights/chief-investment-office/house-view/daily/2023/latest-25052023.html},
}

@misc{modelcontextprotocol2024,
  author    = {Model Context Protocol},
  title     = {{Getting Started - Introduction}},
  year      = {2024},
  url       = {https://modelcontextprotocol.io/docs/getting-started/intro},
}

@article{goodman2001progress,
  author = {Goodman, Joshua},
  title = {{A Bit of Progress in Language Modeling}},
  year = {2001},
  journal = {arXiv preprint},
  eprint = {cs/0108005},
  url = {https://arxiv.org/abs/cs/0108005}
}

@article{kilgarriff2003webascorpus,
  author = {Kilgarriff, Adam and Grefenstette, Gregory},
  title = {{Introduction to the Special Issue on the Web as Corpus}},
  journal = {Computational Linguistics},
  volume = {29},
  number = {3},
  pages = {333--347},
  year = {2003},
  doi = {10.1162/089120103322711569},
  issn = {0891-2017}
}

@inproceedings{banko2001scaling,
  author = {Banko, Michele and Brill, Eric},
  title = {{Scaling to very very large corpora for natural language disambiguation}},
  booktitle = {Proceedings of the 39th Annual Meeting on Association for Computational Linguistics},
  year = {2001},
  pages = {26--33},
  doi = {10.3115/1073012.1073017},
  publisher = {Association for Computational Linguistics}
}

@article{resnik2003webparallel,
  author = {Resnik, Philip and Smith, Noah A.},
  title = {{The Web as a Parallel Corpus}},
  journal = {Computational Linguistics},
  volume = {29},
  number = {3},
  pages = {349--380},
  year = {2003},
  doi = {10.1162/089120103322711578},
  issn = {0891-2017},
  note = {Archived from the original on 2024-06-07. Retrieved 2024-06-07}
}

@inproceedings{xu2000annlm,
  author = {Xu, Wei and Rudnicky, Alex},
  title = {{Can artificial neural networks learn language models?}},
  booktitle = {6th International Conference on Spoken Language Processing (ICSLP 2000)},
  volume = {1},
  year = {2000},
  publisher = {ISCA},
  doi = {10.21437/icslp.2000-50}
}

@article{chen2021cnnreview,
  author = {Chen, Leiyu and Li, Shaobo and Bai, Qiang and Yang, Jing and Jiang, Sanlong and Miao, Yanming},
  title = {{Review of Image Classification Algorithms Based on Convolutional Neural Networks}},
  journal = {Remote Sensing},
  volume = {13},
  number = {22},
  pages = {4712},
  year = {2021},
  doi = {10.3390/rs13224712},
  bibcode = {2021RemS...13.4712C}
}

@inproceedings{vaswani2017attention,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  title = {{Attention is All you Need}},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {30},
  year = {2017},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
  note = {Archived (PDF) from the original on 2024-02-21. Retrieved 2024-01-21}
}

@article{bahdanau2014nmt,
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
  year = {2014},
  journal = {arXiv preprint},
  eprint = {1409.0473},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/1409.0473}
}

@article{rogers2020bertology,
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  title = {{A Primer in BERTology: What We Know About How BERT Works}},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {842--866},
  year = {2020},
  doi = {10.1162/tacl_a_00349},
  eprint = {2002.12327},
  archivePrefix = {arXiv},
  s2cid = {211532403},
  note = {Archived from the original on 2022-04-03. Retrieved 2024-01-21}
}

@inproceedings{movva2024topics,
  author = {Movva, Rajiv and Balachandar, Sidhika and Peng, Kenny and Agostini, Gabriel and Garg, Nikhil and Pierson, Emma},
  title = {{Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers}},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages = {1223--1243},
  year = {2024},
  doi = {10.18653/v1/2024.naacl-long.67},
  eprint = {2307.10700},
  archivePrefix = {arXiv},
  note = {Retrieved 2024-12-08}
}

@article{hern2019fakeai,
  author = {Hern, Alex},
  title = {{New AI fake text generator may be too dangerous to release, say creators}},
  journal = {The Guardian},
  year = {2019},
  month = {February},
  day = {14},
  url = {https://web.archive.org/web/20190214173112/https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction},
  note = {Archived from the original on 14 February 2019. Retrieved 20 January 2024}
}

@article{euronews2023chatgpt,
  author = {Euronews},
  title = {{ChatGPT a year on: 3 ways the AI chatbot has completely changed the world in 12 months}},
  journal = {Euronews},
  year = {2023},
  month = {November},
  day = {30},
  url = {https://www.euronews.com/next/2023/11/30/chatgpt-a-year-on-3-ways-the-ai-chatbot-has-completely-changed-the-world-in-12-months},
  note = {Archived from the original on January 14, 2024. Retrieved January 20, 2024}
}

@article{heaven2023gpt4,
  author = {Heaven, Will},
  title = {{GPT-4 is bigger and better than ChatGPT—but OpenAI won't say why}},
  journal = {MIT Technology Review},
  year = {2023},
  month = {March},
  day = {14},
  url = {https://web.archive.org/web/20230317224201/https://www.technologyreview.com/2023/03/14/1069823/gpt-4-is-bigger-and-better-chatgpt-openai/},
  note = {Archived from the original on March 17, 2023. Retrieved January 20, 2024}
}

@article{metz2024openai,
  author = {Metz, Cade},
  title = {{OpenAI Unveils New ChatGPT That Can Reason Through Math and Science}},
  journal = {The New York Times},
  year = {2024},
  month = {September},
  day = {12},
  url = {https://www.nytimes.com/2024/09/12/technology/openai-chatgpt-math.html},
  note = {Retrieved September 12, 2024}
}

@misc{ourworldindata2023parameters,
  title = {{Parameters in notable artificial intelligence systems}},
  year = {2023},
  month = {November},
  day = {30},
  url = {https://ourworldindata.org/grapher/artificial-intelligence-parameter-count?time=2017-09-05..latest},
  note = {Retrieved January 20, 2024}
}

@article{sharma2025deepseek,
  author = {Sharma, Shubham},
  title = {{Open-source DeepSeek-R1 uses pure reinforcement learning to match OpenAI o1 — at 95\% less cost}},
  journal = {VentureBeat},
  year = {2025},
  month = {January},
  day = {20},
  url = {https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/},
  note = {Retrieved 2025-01-26}
}

@article{zia2024multimodal,
  author = {Zia, Dr Tehseen},
  title = {{Unveiling of Large Multimodal Models: Shaping the Landscape of Language Models in 2024}},
  journal = {Unite.AI},
  year = {2024},
  month = {January},
  day = {8},
  url = {https://www.unite.ai/unveiling-of-large-multimodal-models-shaping-the-landscape-of-language-models-in-2024/},
  note = {Retrieved 2024-12-28}
}

@misc{merritt2022transformer,
  author = {Merritt, Rick},
  title = {{What Is a Transformer Model?}},
  howpublished = {NVIDIA Blog},
  year = {2022},
  month = {March},
  day = {25},
  url = {https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/},
  note = {Archived from the original on 2023-11-17. Retrieved 2023-07-25}
}

@misc{modelcontextprotocol2024intro,
  author    = {Model Context Protocol},
  title     = {{Getting Started - Introduction}},
  year      = {2024},
  url       = {https://modelcontextprotocol.io/docs/getting-started/intro},
  note      = {Accessed: 2025-08-16}
}

@misc{modelcontextprotocol2024arch,
  author    = {Model Context Protocol},
  title     = {{Learn - Architecture}},
  year      = {2024},
  url       = {https://modelcontextprotocol.io/docs/learn/architecture},
  note      = {Accessed: 2025-08-16}
}

@misc{LLM_next_token_prediction,
  title = {{Introduction to Large Language Models}},
  author = {{Google Developers}},
  year = {2024},
  url = {https://developers.google.com/machine-learning/resources/intro-llms}
}

@misc{tokens_tokenization,
  title = {{LLM Transformer Model Visually Explained}},
  author = {Polo Club},
  year = {2024},
  url = {https://poloclub.github.io/transformer-explainer/}
}

@misc{LLM_fine_tuning,
  title = {{What are Large Language Models (LLMs)?}},
  author = {IBM},
  year = {2023},
  url = {https://www.ibm.com/think/topics/large-language-models}
}

@misc{transformer_self_attention,
  title = {{Transformer (deep learning architecture)}},
  author = {{Wikipedia contributors}},
  year = {2025},
  url = {https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)}
}

@misc{attention_human_cognition,
  title = {{What is an attention mechanism?}},
  author = {IBM},
  year = {2024},
  url = {https://www.ibm.com/think/topics/attention-mechanism}
}

@misc{transformer_parallelizable,
  title = {{Attention Is All You Need}},
  author = {{Wikipedia contributors}},
  year = {2025},
  url = {https://en.wikipedia.org/wiki/Attention_Is_All_You_Need}
}

@misc{fine_tuning_transfer_learning,
  title = {{What is fine-tuning?}},
  author = {Dave Bergmann (IBM)},
  year = {2024},
  url = {https://www.ibm.com/think/topics/fine-tuning}
}

@misc{RLHF_reward_model_from_humans,
  title = {{Reinforcement learning from human feedback}},
  author = {{Wikipedia contributors}},
  year = {2025},
  url = {https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback}
}

@misc{azurefunctions_msdocs,
  author = {{Microsoft Docs}},
  title = {{Azure Functions overview}},
  url = {https://learn.microsoft.com/it-it/azure/azure-functions/functions-overview?pivots=programming-language-csharp},
  note = {Accessed: 2025-08-16}
}

@inproceedings{chernyavskiy2021transformers,
  author    = {Chernyavskiy, Andrey and Ilvovsky, Dmitry and Nakov, Preslav},
  title     = {Transformers: "the end of history" for natural language processing?},
  booktitle = {Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13--17, 2021, Proceedings, Part III},
  series    = {Lecture Notes in Computer Science},
  volume    = {12977},
  pages     = {677--693},
  publisher = {Springer},
  year      = {2021},
  doi       = {10.1007/978-3-030-86523-8_41},
  url       = {https://doi.org/10.1007/978-3-030-86523-8_41},
  note      = {Accessed: 2025-09-02}
}

@misc{devlin2019bert,
  author       = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title        = {BERT: Pre-training of deep bidirectional transformers for language understanding},
  year         = {2018},
  eprint       = {arXiv:1810.04805},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/1810.04805},
  note         = {Accessed: 2025-09-02}
}

@inproceedings{peters2018elmo,
  author    = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  title     = {Deep contextualized word representations},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  pages     = {2227--2237},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://aclanthology.org/N18-1202},
  note      = {Accessed: 2025-09-02}
}

@misc{lewis2019bart,
  author       = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  title        = {BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  year         = {2019},
  eprint       = {arXiv:1910.13461},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/1910.13461},
  note         = {Accessed: 2025-09-02}
}

@misc{chung2022scaling,
  author       = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Saurabh and others},
  title        = {Scaling instruction-finetuned language models},
  year         = {2022},
  eprint       = {arXiv:2210.11416},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2210.11416},
  note         = {Accessed: 2025-09-02}
}

@misc{sanh2021multitask,
  author       = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Abheesht and others},
  title        = {Multitask prompted training enables zero-shot task generalization},
  year         = {2021},
  eprint       = {arXiv:2110.08207},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2110.08207},
  note         = {Accessed: 2025-09-02}
}

@inproceedings{wang2022supernaturalinstructions,
  author    = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amir and Naik, Aashka and Ashok, Anjana and Dhanasekaran, Abhinav Suresh and Arunkumar, Anmol and Stap, David and others},
  title     = {Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {5085--5109},
  year      = {2022},
  url       = {https://aclanthology.org/2022.emnlp-main.341},
  note      = {Accessed: 2025-09-02}
}

@misc{wang2022selfinstruct,
  author       = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  title        = {Self-Instruct: Aligning language model with self generated instructions},
  year         = {2022},
  eprint       = {arXiv:2212.10560},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2212.10560},
  note         = {Accessed: 2025-09-02}
}

@inproceedings{ouyang2022rlhf,
  author    = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  title     = {Training language models to follow instructions with human feedback},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {35},
  pages     = {27730--27744},
  year      = {2022},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
  note      = {Accessed: 2025-09-02}
}

@misc{touvron2023llama2,
  author       = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Sharan and Bhargava, Puneet and Bhosale, Shruti and others},
  title        = {LLaMA 2: Open foundation and fine-tuned chat models},
  year         = {2023},
  eprint       = {arXiv:2307.09288},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2307.09288},
  note         = {Accessed: 2025-09-02}
}

@misc{wei2022emergent,
  author       = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  title        = {Emergent abilities of large language models},
  year         = {2022},
  eprint       = {arXiv:2206.07682},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2206.07682},
  note         = {Accessed: 2025-09-02}
}

@article{webb2023emergent,
  author    = {Webb, Taylor and Holyoak, Keith J. and Lu, Hongjing},
  title     = {Emergent analogical reasoning in large language models},
  journal   = {Nature Human Behaviour},
  volume    = {7},
  number    = {9},
  pages     = {1526--1541},
  year      = {2023},
  doi       = {10.1038/s41562-023-01659-w},
  url       = {https://doi.org/10.1038/s41562-023-01659-w},
  note      = {Accessed: 2025-09-02}
}

@misc{boiko2023emergent,
  author       = {Boiko, Daniil A. and MacKnight, Robert and Gomes, Gabriel},
  title        = {Emergent autonomous scientific research capabilities of large language models},
  year         = {2023},
  eprint       = {arXiv:2304.05332},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2304.05332},
  note         = {Accessed: 2025-09-02}
}

@misc{microsoft_webapi_ef_part5,
  author       = {{Microsoft Docs}},
  title        = {{Using Web API with Entity Framework (Part 5)}},
  howpublished = {Microsoft Docs},
  url          = {https://learn.microsoft.com/it-it/aspnet/web-api/overview/data/using-web-api-with-entity-framework/part-5},
  note         = {Accessed: 2025-09-02}
}

@misc{azure_cqrs_msdocs,
  author       = {{Microsoft Docs}},
  title        = {{CQRS pattern}},
  howpublished = {Microsoft Docs},
  url          = {https://learn.microsoft.com/it-it/azure/architecture/patterns/cqrs},
  note         = {Accessed: 2025-09-02}
}